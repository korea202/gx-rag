{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 파일 생성 파트1 (답변 중간파일 생성 all_data.pkl)\n",
    "# llm 변경후 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 파일 읽기\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_csv(\"./data/summary_one.csv\")\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 정보 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af36c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디비 생성 클래스\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_empty_faiss(embeddings, use_cosine=True):\n",
    "    \"\"\"빈 FAISS 벡터스토어 생성\"\"\"\n",
    "    dimension = len(embeddings.embed_query(\"test\"))\n",
    "    print(f\"dimension={dimension}\")\n",
    "    if use_cosine:\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        normalize = True\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        normalize = False\n",
    "    \n",
    "    return FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=normalize\n",
    "    )\n",
    "\n",
    "def create_empty_qdrant(embeddings):\n",
    "\n",
    "    client = QdrantClient(\":memory:\")\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=\"demo_collection\",\n",
    "        vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    return QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"demo_collection\",\n",
    "        embedding=embeddings,\n",
    "    )  \n",
    "\n",
    "def get_embeddinggemma_300m(opt):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"google/embeddinggemma-300m\",\n",
    "        encode_kwargs={\"prompt_name\": opt}\n",
    "    )\n",
    "\n",
    "def get_qwen3_embedding_4b():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"Qwen/Qwen3-Embedding-4B\",\n",
    "        model_kwargs={\n",
    "            \"device\": \"cuda\"\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_bge_m3():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-m3\",\n",
    "        #model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        model_kwargs={\"device\": \"cuda\"} ,\n",
    "        encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "class ScienceRAG:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.embeddings = get_bge_m3()\n",
    "        \n",
    "        self.vectorstore = create_empty_faiss(self.embeddings, use_cosine=True) \n",
    "        \n",
    "        # 요약 체인\n",
    "        #self.summary_chain = science_summary_chain\n",
    "        #self.summary_chain = two_step_chain\n",
    "    \n",
    "    def add_documents(self, df):\n",
    "        \"\"\"문서를 요약하여 벡터DB에 저장\"\"\"\n",
    "        \n",
    "        texts = df['summary'].tolist()\n",
    "        metadatas = [\n",
    "            {\n",
    "                'docid': row['docid'],\n",
    "                'content': row['content']\n",
    "            } \n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        ids = df['docid'].tolist()\n",
    "\n",
    "\n",
    "        # 2. 검색용 요약을 벡터DB에 저장\n",
    "        self.vectorstore.add_texts(\n",
    "            texts,\n",
    "            metadatas=metadatas,\n",
    "            ids = ids\n",
    "        )\n",
    "        \n",
    "       \n",
    "    def search(self, query: str, k: int = 3):\n",
    "        \"\"\"요약된 내용으로 검색\"\"\"\n",
    "        results = self.vectorstore.similarity_search(query, k=k)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 디비 생성 \n",
    "\n",
    "science_rag = ScienceRAG()\n",
    "\n",
    "science_rag.add_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "query = \"공에 힘이 주어졌을 때 공이 어떻게 움직이는지 과학적으로 설명해줘.\"\n",
    "docs = science_rag.vectorstore.similarity_search_with_score(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 질의문 생성 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"alibayram/Qwen3-30B-A3B-Instruct-2507\")\n",
    "\n",
    "# 프롬프트 \n",
    "convertFormat = \"\"\"\n",
    "당신은 문자열 포맷 마이그레이션 전문가 입니다. <message> 에서 content 내용만 문자열로 출력합니다. 만일 content가 여러개가 있으면 전체적인 문맥을 파악하여 질문을 만들어서 출력합니다.\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "[example]\n",
    "\n",
    "    <message>{{\"role\": \"user\", \"content\": \"피를 맑게 하고 몸 속의 노폐물을 없애는 역할을 하는 기관은?\"}}</message> \n",
    "    output:피를 맑게 하고 몸 속의 노폐물을 없애는 역할을 하는 기관은? \n",
    "\n",
    "    <message>{{\"role\": \"user\", \"content\": \"이란 콘트라 사건이 뭐야\"}}, {{\"role\": \"assistant\", \"content\": \"이란-콘트라 사건은 로널드 레이건 집권기인 1986년에 레이건 행정부와 CIA가 적성국이었던 이란에게 무기를 몰래 수출한 대금으로 니카라과의 우익 성향 반군 콘트라를 지원하면서 동시에 반군으로부터 마약을 사들인 후 미국에 판매하다가 발각되어 큰 파장을 일으킨 사건입니다.\"}}, {{\"role\": \"user\", \"content\": \"이 사건이 미국 정치에 미친 영향은?\"}}</message>\n",
    "    output:1986년에 발생한 이란 콘트라 사건이 미국 정치에 미친 영향은? \n",
    "\n",
    "\n",
    "output:\n",
    "[Your output here - NOTHING ELSE]\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "convertFormat_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=convertFormat\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "convertFormat_chain = (\n",
    "    convertFormat_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 과학상식 체크 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# 프롬프트 \n",
    "selectYn = \"\"\"\n",
    "당신은 세상의 모든 상식과 지식에 정통한 전문가 입니다. 만약 <message> 가 세상의 상식 또는 지식에 관련된 질문이라면  Y 아니라면 N으로 답해주세요. 너에 관하여 물어보는건 세상의 상식 또는 지식에 관련된 질문이 아니야!\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "당신은 반드시 Y 뜨는 N으로 답해야 합니다.\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "selectYn_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=selectYn\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "selectYn_chain = (\n",
    "    selectYn_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 답변 생성 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# 프롬프트 \n",
    "answer = \"\"\"\n",
    "당신은 과학 상식 전문가 입니다. <message> 의 질문에 대해서 주어진 <reference> 정보를 활용하여 간결하게 답변을 생성합니다.\n",
    "\n",
    "    - 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답합니다.\n",
    "    - 한국어로 답변을 생성합니다..\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "<reference>\n",
    "{reference}\n",
    "</reference>\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=answer\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "answer_chain = (\n",
    "    answer_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 데이터 검색 함수\n",
    "\n",
    "def query_db(message, k=30):\n",
    "\n",
    "    docs = science_rag.vectorstore.similarity_search_with_relevance_scores(message, k=k)\n",
    "\n",
    "    content = []\n",
    "    docid= []\n",
    "    reference = []\n",
    "    \n",
    "    for doc, score in docs:\n",
    "        \n",
    "        content.append(doc.metadata['content'])\n",
    "        docid.append(doc.metadata['docid'])\n",
    "        reference.append({\"score\": float(score), \"docid\": doc.metadata['docid'], \"content\": doc.metadata['content']})\n",
    "\n",
    "    return content[:3], docid, reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77380f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "message = convertFormat_chain.invoke({\"message\": '{\"role\": \"user\", \"content\": \"python 공부중인데...\"}, {\"role\": \"assistant\", \"content\": \"네 꼭 필요한 언어라서 공부해 두면 좋습니다.\"}, {\"role\": \"user\", \"content\": \"숫자 계산을 위한 operator 우선순위에 대해 알려줘.\"}'})\n",
    "print(message)\n",
    "result = selectYn_chain.invoke({\"message\": \"니가 대답을 잘해줘서 너무 신나!\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ea0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 메인 로직\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "from FlagEmbedding import FlagLLMReranker\n",
    "\n",
    "# 답변 데이터 생성\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    \n",
    "    message = convertFormat_chain.invoke({\"message\": messages})\n",
    "    result = selectYn_chain.invoke({\"message\": message})\n",
    "\n",
    "    context = {\"message\":message}\n",
    "    response =  context | {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "    \n",
    "    if result == \"Y\":\n",
    "        context[\"reference\"], response[\"topk\"], response[\"references\"] = query_db(message)\n",
    "    else:\n",
    "        context[\"reference\"] = \"\"\n",
    "        response[\"topk\"] = []\n",
    "        response[\"references\"] = []\n",
    "    \n",
    "    response[\"answer\"] = answer_chain.invoke(context)\n",
    "\n",
    "    return response\n",
    "\n",
    "def reranking(data):\n",
    "\n",
    "    reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True)\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    references = data[\"references\"]\n",
    "    pairs = [[message, doc['content']] for doc in references]\n",
    "    rerank_scores = reranker.compute_score(pairs)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(pairs)\n",
    "    print(references)\n",
    "    print(rerank_scores)\n",
    "\n",
    "\n",
    "    results = list(zip(references, rerank_scores))\n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    final_result = results[:3]\n",
    "    print(f\"final_result={final_result}\")\n",
    "\n",
    "    data['topk'] = [ doc['docid'] for doc, socre in final_result]\n",
    "    data[\"references\"] = [ {\"score\": float(score), \"content\": doc['content']} for doc, score in final_result ]\n",
    "\n",
    "    return data\n",
    "\n",
    "def eval_rag(eval_filename):\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    with open(eval_filename) as f:\n",
    "        idx = 0\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            #if idx == 20: break\n",
    "            \n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"message\": response[\"message\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            #print(output)\n",
    "            all_data.append(output)\n",
    "            idx += 1\n",
    "\n",
    "        \n",
    "    return all_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def eval_rag2(all_data, output_filename):\n",
    "\n",
    "    with open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for data in all_data:\n",
    "            print(f\"data={data}\")\n",
    "            if len(data[\"topk\"]) > 0:\n",
    "                response = reranking(data)\n",
    "\n",
    "            print(f'response: {response}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": response[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            print(output)\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d204b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 메인 \n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성\n",
    "all_data = eval_rag(\"./data/eval.jsonl\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae71905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올라마 프로세스 종료\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['pkill', '-9', '-f', 'ollama'])\n",
    "\n",
    "# GPU 메모리 확인\n",
    "subprocess.run(['nvidia-smi'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 저장\n",
    "import pickle\n",
    "\n",
    "with open('./data/all_data.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_rag2(all_data, \"./data/sample_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
