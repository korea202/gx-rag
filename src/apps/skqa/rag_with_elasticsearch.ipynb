{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Sentence Transformer 모델 초기화 (한국어 임베딩 생성 가능한 어떤 모델도 가능)\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb833a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_username = \"elastic\"\n",
    "es_password = \"AdLlo06jlWDRMwS+e4*U\"\n",
    "\n",
    "# Elasticsearch client 생성\n",
    "es = Elasticsearch(['https://localhost:9200'], basic_auth=(es_username, es_password), ca_certs=\"/data/ephemeral/home/work/elasticsearch-8.8.0/config/certs/http_ca.crt\")\n",
    "\n",
    "# Elasticsearch client 정보 확인\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SetntenceTransformer를 이용하여 임베딩 생성\n",
    "def get_embedding(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "\n",
    "# 주어진 문서의 리스트에서 배치 단위로 임베딩 생성\n",
    "def get_embeddings_in_batches(docs, batch_size=100):\n",
    "    batch_embeddings = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        contents = [doc[\"content\"] for doc in batch]\n",
    "        embeddings = get_embedding(contents)\n",
    "        batch_embeddings.extend(embeddings)\n",
    "        print(f'batch {i}')\n",
    "    return batch_embeddings\n",
    "\n",
    "\n",
    "# 새로운 index 생성\n",
    "def create_es_index(index, settings, mappings):\n",
    "    # 인덱스가 이미 존재하는지 확인\n",
    "    if es.indices.exists(index=index):\n",
    "        # 인덱스가 이미 존재하면 설정을 새로운 것으로 갱신하기 위해 삭제\n",
    "        es.indices.delete(index=index)\n",
    "    # 지정된 설정으로 새로운 인덱스 생성\n",
    "    es.indices.create(index=index, settings=settings, mappings=mappings)\n",
    "\n",
    "\n",
    "# 지정된 인덱스 삭제\n",
    "def delete_es_index(index):\n",
    "    es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# Elasticsearch 헬퍼 함수를 사용하여 대량 인덱싱 수행\n",
    "def bulk_add(index, docs):\n",
    "    # 대량 인덱싱 작업을 준비\n",
    "    actions = [\n",
    "        {\n",
    "            '_index': index,\n",
    "            '_source': doc\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return helpers.bulk(es, actions)\n",
    "\n",
    "\n",
    "# 역색인을 이용한 검색\n",
    "def sparse_retrieve(query_str, size):\n",
    "    query = {\n",
    "        \"match\": {\n",
    "            \"content\": {\n",
    "                \"query\": query_str\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es.search(index=\"test\", query=query, size=size, sort=\"_score\")\n",
    "\n",
    "\n",
    "# Vector 유사도를 이용한 검색\n",
    "def dense_retrieve(query_str, size):\n",
    "    # 벡터 유사도 검색에 사용할 쿼리 임베딩 가져오기\n",
    "    query_embedding = get_embedding([query_str])[0]\n",
    "\n",
    "    # KNN을 사용한 벡터 유사성 검색을 위한 매개변수 설정\n",
    "    knn = {\n",
    "        \"field\": \"embeddings\",\n",
    "        \"query_vector\": query_embedding.tolist(),\n",
    "        \"k\": size,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "\n",
    "    # 지정된 인덱스에서 벡터 유사도 검색 수행\n",
    "    return es.search(index=\"test\", knn=knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색인을 위한 setting 설정\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"nori\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"nori_tokenizer\",\n",
    "                \"decompound_mode\": \"mixed\",\n",
    "                \"filter\": [\"nori_posfilter\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"nori_posfilter\": {\n",
    "                \"type\": \"nori_part_of_speech\",\n",
    "                # 어미, 조사, 구분자, 줄임표, 지정사, 보조 용언 등\n",
    "                \"stoptags\": [\"E\", \"J\", \"SC\", \"SE\", \"SF\", \"VCN\", \"VCP\", \"VX\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 색인을 위한 mapping 설정 (역색인 필드, 임베딩 필드 모두 설정)\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\"type\": \"text\", \"analyzer\": \"nori\"},\n",
    "        \"embeddings\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# settings, mappings 설정된 내용으로 'test' 인덱스 생성\n",
    "create_es_index(\"test\", settings, mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서의 content 필드에 대한 임베딩 생성\n",
    "index_docs = []\n",
    "with open(\"./data/documents.jsonl\") as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "embeddings = get_embeddings_in_batches(docs)\n",
    "                \n",
    "# 생성한 임베딩을 색인할 필드로 추가\n",
    "for doc, embedding in zip(docs, embeddings):\n",
    "    doc[\"embeddings\"] = embedding.tolist()\n",
    "    index_docs.append(doc)\n",
    "\n",
    "# 'test' 인덱스에 대량 문서 추가\n",
    "ret = bulk_add(\"test\", index_docs)\n",
    "\n",
    "# 색인이 잘 되었는지 확인 (색인된 총 문서수가 출력되어야 함)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"금성이 다른 행성들보다 밝게 보이는 이유는 무엇인가요?\"\n",
    "\n",
    "# 역색인을 사용하는 검색 예제\n",
    "search_result_retrieve = sparse_retrieve(test_query, 3)\n",
    "\n",
    "# 결과 출력 테스트\n",
    "for rst in search_result_retrieve['hits']['hits']:\n",
    "    print('score:', rst['_score'], 'source:', rst['_source'][\"content\"])\n",
    "\n",
    "# Vector 유사도 사용한 검색 예제\n",
    "search_result_retrieve = dense_retrieve(test_query, 3)\n",
    "\n",
    "# 결과 출력 테스트\n",
    "for rst in search_result_retrieve['hits']['hits']:\n",
    "    print('score:', rst['_score'], 'source:', rst['_source'][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래부터는 실제 RAG를 구현하는 코드입니다.\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "# OpenAI API 키를 환경변수에 설정\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"Your API Key\"\n",
    "\n",
    "#client = OpenAI()\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'  # 필수이지만 실제로는 사용되지 않음\n",
    ")\n",
    "\n",
    "# 사용할 모델을 설정\n",
    "#llm_model = \"gpt-4o-mini\"\n",
    "llm_model = \"alibayram/Qwen3-30B-A3B-Instruct-2507\"\n",
    "\n",
    "# RAG 구현에 필요한 Question Answering을 위한 LLM  프롬프트\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# RAG 구현에 필요한 질의 분석 및 검색 이외의 일반 질의 대응을 위한 LLM 프롬프트\n",
    "persona_function_calling = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instruction\n",
    "- 사용자가 대화를 통해 과학 지식에 관한 주제로 질문하면 search api를 호출할 수 있어야 한다.\n",
    "- 과학 상식과 관련되지 않은 나머지 대화 메시지에는 적절한 대답을 생성한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling에 사용할 함수 정의\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"search relevant documents\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"standalone_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Final query suitable for use in search from the user messages history.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"standalone_query\"],\n",
    "                \"type\": \"object\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# LLM과 검색엔진을 활용한 RAG 구현\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    # 질의 분석 및 검색 이외의 질의 대응을 위한 LLM 활용\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages\n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=msg,\n",
    "            tools=tools,\n",
    "            #tool_choice={\"type\": \"function\", \"function\": {\"name\": \"search\"}},\n",
    "            temperature=0,\n",
    "            seed=1,\n",
    "            timeout=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return response\n",
    "\n",
    "    # 검색이 필요한 경우 검색 호출후 결과를 활용하여 답변 생성\n",
    "    if result.choices[0].message.tool_calls:\n",
    "        tool_call = result.choices[0].message.tool_calls[0]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        standalone_query = function_args.get(\"standalone_query\")\n",
    "\n",
    "        # Baseline으로는 sparse_retrieve만 사용하여 검색 결과 추출\n",
    "        search_result = sparse_retrieve(standalone_query, 3)\n",
    "\n",
    "        response[\"standalone_query\"] = standalone_query\n",
    "        retrieved_context = []\n",
    "        for i,rst in enumerate(search_result['hits']['hits']):\n",
    "            retrieved_context.append(rst[\"_source\"][\"content\"])\n",
    "            response[\"topk\"].append(rst[\"_source\"][\"docid\"])\n",
    "            response[\"references\"].append({\"score\": rst[\"_score\"], \"content\": rst[\"_source\"][\"content\"]})\n",
    "\n",
    "        content = json.dumps(retrieved_context)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n",
    "        try:\n",
    "            qaresult = client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=msg,\n",
    "                    temperature=0,\n",
    "                    seed=1,\n",
    "                    timeout=30\n",
    "                )\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return response\n",
    "        response[\"answer\"] = qaresult.choices[0].message.content\n",
    "\n",
    "    # 검색이 필요하지 않은 경우 바로 답변 생성\n",
    "    else:\n",
    "        response[\"answer\"] = result.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"./data/eval.jsonl\", \"sample_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
