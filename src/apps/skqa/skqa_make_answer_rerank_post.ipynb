{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf576d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 파일 생성 파트2 (sample_submission.csv 생성)\n",
    "# 리랭커 변경후 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eabbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 파일 읽기\n",
    "\n",
    "import pickle\n",
    "with open('./data/all_data.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "#print(loaded_data) \n",
    "print(loaded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1   Qwen/Qwen3-Reranker-8B 리랭커 사용시 루트\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "        instruction=instruction, query=query, doc=doc\n",
    "    )\n",
    "    return output\n",
    "\n",
    "class QwenReranker:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-Reranker-8B\", use_flash_attention=False, device=None):\n",
    "        print(f\"Qwen Reranker 모델 로딩 중: {model_name}\")\n",
    "        \n",
    "        # 토크나이저 로딩\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "        \n",
    "        # 모델 로딩\n",
    "        if use_flash_attention:\n",
    "            # device_map=\"auto\"를 사용하면 모델이 여러 GPU에 자동으로 분산됩니다.\n",
    "            # 이때는 모델 전체를 특정 device로 다시 옮기지 않습니다.\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.float16, \n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                device_map=\"auto\"\n",
    "            ).eval()\n",
    "            # 입력 텐서를 배치할 기본 장치 설정. 보통 cuda:0이 됩니다.\n",
    "            # model.device를 사용하면 분산 모델의 경우 'cpu'를 반환할 수 있으므로, 명시적으로 설정\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"모델이 'auto' device_map으로 로딩되어 여러 GPU에 분산되었습니다. 입력은 {self.device}로 이동됩니다.\")\n",
    "        else:\n",
    "            # Flash Attention을 사용하지 않는 경우, 모델을 지정된 단일 장치로 옮깁니다.\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).eval()\n",
    "            # 디바이스 설정\n",
    "            if device is None:\n",
    "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.device = device\n",
    "            self.model = self.model.to(self.device) # 모델을 단일 장치로 이동\n",
    "            print(f\"모델이 {self.device}로 로딩되었습니다.\")\n",
    "        \n",
    "        # 토큰 ID 설정\n",
    "        self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        self.max_length = 8192\n",
    "        \n",
    "        # 프리픽스와 서픽스 설정\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "            \n",
    "    def process_inputs(self, pairs):\n",
    "        \"\"\"입력 텍스트들을 토크나이즈하고 처리\"\"\"\n",
    "        # attention_mask도 함께 반환하도록 설정\n",
    "        inputs = self.tokenizer(\n",
    "            pairs, \n",
    "            padding=False, # initial padding=False\n",
    "            truncation='longest_first',\n",
    "            return_attention_mask=True, # attention_mask를 반환하도록 설정\n",
    "            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
    "        )\n",
    "        \n",
    "        # 프리픽스와 서픽스 추가 및 attention_mask 업데이트\n",
    "        for i in range(len(inputs['input_ids'])):\n",
    "            original_input_ids = inputs['input_ids'][i]\n",
    "            original_attention_mask = inputs['attention_mask'][i]\n",
    "\n",
    "            inputs['input_ids'][i] = self.prefix_tokens + original_input_ids + self.suffix_tokens\n",
    "            # 새로 추가된 토큰에 맞춰 attention_mask 갱신\n",
    "            inputs['attention_mask'][i] = [1] * len(self.prefix_tokens) + original_attention_mask + [1] * len(self.suffix_tokens)\n",
    "        \n",
    "        # 최종 패딩 (input_ids와 attention_mask 모두 처리)\n",
    "        inputs = self.tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return inputs\n",
    "\n",
    "    def compute_logits(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            batch_scores = self.model(**inputs).logits[:, -1, :]\n",
    "            true_vector = batch_scores[:, self.token_true_id]\n",
    "            false_vector = batch_scores[:, self.token_false_id]\n",
    "            batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "            scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "    \n",
    "    def rerank_documents(self, query, documents, batch_size=32, task=None):\n",
    "        if task is None:\n",
    "            task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        \n",
    "        # 모든 쿼리-문서 쌍을 형식화\n",
    "        pairs = [format_instruction(task, query, doc) for doc in documents]\n",
    "        \n",
    "        all_scores = []\n",
    "        \n",
    "        # 배치 단위로 처리\n",
    "        for i in tqdm(range(0, len(pairs), batch_size), desc=\"문서 재순위 매기는 중\"):\n",
    "            batch_pairs = pairs[i:i + batch_size]\n",
    "            inputs = self.process_inputs(batch_pairs)\n",
    "            scores = self.compute_logits(inputs)\n",
    "            all_scores.extend(scores)\n",
    "        \n",
    "        return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2   Qwen/Qwen3-Reranker-8B 리랭커 사용시 루트\n",
    "\n",
    "# 리스트 불러오기\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def reranking(data):\n",
    "\n",
    "    # Reranker 초기화\n",
    "    reranker = QwenReranker(\n",
    "        model_name='Qwen/Qwen3-Reranker-8B',\n",
    "        use_flash_attention=False\n",
    "    )\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    references = data[\"references\"]\n",
    "    documents = [doc['content'] for doc in references]\n",
    "    \n",
    "    rerank_scores = reranker.rerank_documents(message, documents, batch_size=1)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(references)\n",
    "    print(rerank_scores)\n",
    "\n",
    "    results = list(zip(references, rerank_scores))\n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    final_result = results[:3]\n",
    "    print(f\"final_result={final_result}\")\n",
    "\n",
    "    data['topk'] = [ doc['docid'] for doc, socre in final_result]\n",
    "    data[\"references\"] = [ {\"score\": float(score), \"content\": doc['content']} for doc, score in final_result ]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./data/sample_submission.csv\", \"w\") as of:\n",
    "        idx = 0\n",
    "        for data in loaded_data:\n",
    "\n",
    "            #if idx == 10: break;\n",
    "\n",
    "            print(f\"data={data}\")\n",
    "            if len(data[\"topk\"]) > 0:\n",
    "                response = reranking(data)\n",
    "\n",
    "            print(f'response: {response}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": response[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            print(output)\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d76b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3   기타 리랭커 사용시 루트\n",
    "\n",
    "# 리스트 불러오기\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from FlagEmbedding import FlagLLMReranker, FlagReranker \n",
    "\n",
    "def reranking(data):\n",
    "\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-v2-m3')\n",
    "    #reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True)\n",
    "    #reranker = FlagLLMReranker('Qwen/Qwen3-Reranker-8B', use_fp16=True, batch_size=1, max_length=4096)\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    references = data[\"references\"]\n",
    "    pairs = [[message, doc['content']] for doc in references]\n",
    "    rerank_scores = reranker.compute_score(pairs)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(pairs)\n",
    "    print(references)\n",
    "    print(rerank_scores)\n",
    "\n",
    "\n",
    "    results = list(zip(references, rerank_scores))\n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    final_result = results[:3]\n",
    "    print(f\"final_result={final_result}\")\n",
    "\n",
    "    data['topk'] = [ doc['docid'] for doc, socre in final_result]\n",
    "    data[\"references\"] = [ {\"score\": float(score), \"content\": doc['content']} for doc, score in final_result ]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with open(\"./data/sample_submission.csv\", \"w\") as of:\n",
    "        idx = 0\n",
    "        for data in loaded_data:\n",
    "            print(f\"data={data}\")\n",
    "            if len(data[\"topk\"]) > 0:\n",
    "                response = reranking(data)\n",
    "\n",
    "            print(f'response: {response}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": response[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            print(output)\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4  리랭커 앙상블 사용시 루트\n",
    "\n",
    "# 리스트 불러오기\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from FlagEmbedding import FlagLLMReranker, FlagReranker \n",
    "\n",
    "def rank_based_ensemble(score_cross, score_llm, weights=[0.5, 0.5]):\n",
    "    \"\"\"랭크를 기반으로 앙상블\"\"\"\n",
    "    \n",
    "    # 각 스코어를 랭크로 변환 (높은 점수 = 낮은 랭크)\n",
    "    rank_cross = np.argsort(np.argsort(-np.array(score_cross)))\n",
    "    rank_llm = np.argsort(np.argsort(-np.array(score_llm)))\n",
    "    \n",
    "    # 랭크를 0-1로 정규화\n",
    "    n = len(rank_cross)\n",
    "    norm_rank_cross = rank_cross / (n - 1)\n",
    "    norm_rank_llm = rank_llm / (n - 1)\n",
    "    \n",
    "    # 가중 평균\n",
    "    final_ranks = weights[0] * norm_rank_cross + weights[1] * norm_rank_llm\n",
    "    \n",
    "    # 최종 스코어로 변환 (낮은 랭크 = 높은 스코어)\n",
    "    return 1 - final_ranks\n",
    "\n",
    "def reranking(data):\n",
    "\n",
    "    reranker_cross = FlagReranker('BAAI/bge-reranker-v2-m3', batch_size=8)\n",
    "    reranker_llm = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', batch_size=8)\n",
    "    \n",
    "\n",
    "    message = data[\"message\"]\n",
    "    references = data[\"references\"]\n",
    "    pairs = [[message, doc['content']] for doc in references]\n",
    "    score_cross = reranker_cross.compute_score(pairs)\n",
    "    score_llm = reranker_llm.compute_score(pairs)\n",
    "\n",
    "    rerank_scores = rank_based_ensemble(score_cross, score_llm)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(pairs)\n",
    "    print(references)\n",
    "    print(rerank_scores)\n",
    "\n",
    "\n",
    "    results = list(zip(references, rerank_scores))\n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    final_result = results[:3]\n",
    "    print(f\"final_result={final_result}\")\n",
    "\n",
    "    data['topk'] = [ doc['docid'] for doc, socre in final_result]\n",
    "    data[\"references\"] = [ {\"score\": float(score), \"content\": doc['content']} for doc, score in final_result ]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with open(\"./data/sample_submission.csv\", \"w\") as of:\n",
    "        idx = 0\n",
    "        for data in loaded_data:\n",
    "            print(f\"data={data}\")\n",
    "            if len(data[\"topk\"]) > 0:\n",
    "                response = reranking(data)\n",
    "\n",
    "            print(f'response: {response}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": response[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            print(output)\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
