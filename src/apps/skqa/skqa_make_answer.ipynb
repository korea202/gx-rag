{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 파일 생성(sample_submission.csv)\n",
    "# llm 변경후 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91310582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 파일 읽기\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_csv(\"./data/summary_one.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 정보 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af36c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디비 생성 클래스\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "def create_empty_faiss(embeddings, use_cosine=True):\n",
    "    \"\"\"빈 FAISS 벡터스토어 생성\"\"\"\n",
    "    dimension = len(embeddings.embed_query(\"test\"))\n",
    "    print(f\"dimension={dimension}\")\n",
    "    if use_cosine:\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        normalize = True\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        normalize = False\n",
    "    \n",
    "    return FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=normalize\n",
    "    )\n",
    "\n",
    "def get_embeddinggemma_300m(opt):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"google/embeddinggemma-300m\",\n",
    "        encode_kwargs={\"prompt_name\": opt}\n",
    "    )\n",
    "\n",
    "def get_qwen3_embedding_4b():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"Qwen/Qwen3-Embedding-4B\",\n",
    "        model_kwargs={\n",
    "            \"device\": \"cuda\"\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_bge_m3():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-m3\",\n",
    "        #model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        model_kwargs={\"device\": \"cuda\"} ,\n",
    "        encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "class ScienceRAG:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.embeddings = get_bge_m3()\n",
    "        \n",
    "        self.vectorstore = create_empty_faiss(self.embeddings, use_cosine=True) \n",
    "        \n",
    "        # 요약 체인\n",
    "        #self.summary_chain = science_summary_chain\n",
    "        #self.summary_chain = two_step_chain\n",
    "    \n",
    "    def add_documents(self, df):\n",
    "        \"\"\"문서를 요약하여 벡터DB에 저장\"\"\"\n",
    "        \n",
    "        texts = df['summary'].tolist()\n",
    "        metadatas = [\n",
    "            {\n",
    "                'docid': row['docid'],\n",
    "                'content': row['content']\n",
    "            } \n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        ids = df['docid'].tolist()\n",
    "\n",
    "\n",
    "        # 2. 검색용 요약을 벡터DB에 저장\n",
    "        self.vectorstore.add_texts(\n",
    "            texts,\n",
    "            metadatas=metadatas,\n",
    "            ids = ids\n",
    "        )\n",
    "        \n",
    "       \n",
    "    def search(self, query: str, k: int = 3):\n",
    "        \"\"\"요약된 내용으로 검색\"\"\"\n",
    "        results = self.vectorstore.similarity_search(query, k=k)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 디비 생성 \n",
    "\n",
    "science_rag = ScienceRAG()\n",
    "\n",
    "science_rag.add_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "query = \"공에 힘이 주어졌을 때 공이 어떻게 움직이는지 과학적으로 설명해줘.\"\n",
    "docs = science_rag.vectorstore.similarity_search_with_score(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 질의문 생성 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"alibayram/Qwen3-30B-A3B-Instruct-2507\")\n",
    "\n",
    "# 프롬프트 \n",
    "convertFormat = \"\"\"\n",
    "당신은 문자열 포맷 마이그레이션 전문가 입니다. <message> 에서 content 내용만 문자열로 출력합니다. 만일 content가 여러개가 있으면 전체적인 문맥을 파악하여 질문을 만들어서 출력합니다.\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "[example]\n",
    "\n",
    "    <message>{{\"role\": \"user\", \"content\": \"피를 맑게 하고 몸 속의 노폐물을 없애는 역할을 하는 기관은?\"}}</message> \n",
    "    output:피를 맑게 하고 몸 속의 노폐물을 없애는 역할을 하는 기관은? \n",
    "\n",
    "    <message>{{\"role\": \"user\", \"content\": \"이란 콘트라 사건이 뭐야\"}}, {{\"role\": \"assistant\", \"content\": \"이란-콘트라 사건은 로널드 레이건 집권기인 1986년에 레이건 행정부와 CIA가 적성국이었던 이란에게 무기를 몰래 수출한 대금으로 니카라과의 우익 성향 반군 콘트라를 지원하면서 동시에 반군으로부터 마약을 사들인 후 미국에 판매하다가 발각되어 큰 파장을 일으킨 사건입니다.\"}}, {{\"role\": \"user\", \"content\": \"이 사건이 미국 정치에 미친 영향은?\"}}</message>\n",
    "    output:1986년에 발생한 이란 콘트라 사건이 미국 정치에 미친 영향은? \n",
    "\n",
    "\n",
    "output:\n",
    "[Your output here - NOTHING ELSE]\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "convertFormat_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=convertFormat\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "convertFormat_chain = (\n",
    "    convertFormat_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 과학상식 체크 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# 프롬프트 \n",
    "selectYn = \"\"\"\n",
    "당신은 세상의 모든 상식과 지식에 정통한 전문가 입니다. 만약 <message> 가 세상의 상식 또는 지식에 관련된 질문이라면  Y 아니라면 N으로 답해주세요. 너에 관하여 물어보는건 세상의 상식 또는 지식에 관련된 질문이 아니야!\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "당신은 반드시 Y 뜨는 N으로 답해야 합니다.\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "selectYn_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=selectYn\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "selectYn_chain = (\n",
    "    selectYn_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 답변 생성 체인\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# 프롬프트 \n",
    "answer = \"\"\"\n",
    "당신은 과학 상식 전문가 입니다. <message> 의 질문에 대해서 주어진 <reference> 정보를 활용하여 간결하게 답변을 생성합니다.\n",
    "\n",
    "    - 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답합니다.\n",
    "    - 한국어로 답변을 생성합니다..\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "<reference>\n",
    "{reference}\n",
    "</reference>\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 객체 생성\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=answer\n",
    ")\n",
    "\n",
    "# 출력 파서 (문자열)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL 체인 구성\n",
    "answer_chain = (\n",
    "    answer_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 데이터 검색 함수\n",
    "\n",
    "def query_db(message):\n",
    "\n",
    "    docs = science_rag.vectorstore.similarity_search_with_relevance_scores(message, k=3)\n",
    "\n",
    "    #filtered_docs = []\n",
    "    content = []\n",
    "    docid= []\n",
    "    reference = []\n",
    "    \n",
    "    for doc, score in docs:\n",
    "        #if score <= 0.8:    \n",
    "            #filtered_docs.append((doc, score))    \n",
    "            content.append(doc.metadata['content'])\n",
    "            docid.append(doc.metadata['docid'])\n",
    "            reference.append({\"score\": float(score), \"content\": doc.metadata['content']})\n",
    "    return content, docid, reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77380f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "message = convertFormat_chain.invoke({\"message\": '{\"role\": \"user\", \"content\": \"python 공부중인데...\"}, {\"role\": \"assistant\", \"content\": \"네 꼭 필요한 언어라서 공부해 두면 좋습니다.\"}, {\"role\": \"user\", \"content\": \"숫자 계산을 위한 operator 우선순위에 대해 알려줘.\"}'})\n",
    "print(message)\n",
    "result = selectYn_chain.invoke({\"message\": \"니가 대답을 잘해줘서 너무 신나!\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91573a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "#message = convertFormat_chain.invoke({\"message\": '{\"role\": \"user\", \"content\": \"기억 상실증 걸리면 너무 무섭겠다.\"}, {\"role\": \"assistant\", \"content\": \"네 맞습니다.\"}, {\"role\": \"user\", \"content\": \"어떤 원인 때문에 발생하는지 궁금해.\"}'})\n",
    "message = convertFormat_chain.invoke({\"message\": '복숭아 키우는 노하우좀?'})\n",
    "\n",
    "response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "context = {\"message\":message}\n",
    "\n",
    "result = selectYn_chain.invoke({\"message\": message})\n",
    "\n",
    "if result == \"Y\":\n",
    "    context[\"reference\"], response[\"topk\"], response[\"references\"] = query_db(message)\n",
    "else:\n",
    "    context[\"reference\"] = \"\"\n",
    "    response[\"topk\"] = []\n",
    "    response[\"references\"] = []\n",
    "\n",
    "# 전역 디버그 모드 활성화\n",
    "set_debug(True)\n",
    "\n",
    "response[\"answer\"] = answer_chain.invoke(context)\n",
    "print(response[\"topk\"])\n",
    "print(response[\"answer\"])\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339542df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import json\n",
    "\n",
    "with open(\"./data/eval.jsonl\") as f:\n",
    "   for line in f:\n",
    "\n",
    "            j = json.loads(line)\n",
    "            message = convertFormat_chain.invoke({\"message\": j[\"msg\"]})\n",
    "            print(f'{message}:{len(message)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac10f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "content, docid, reference = query_db(\"세제의 거품이 만들어지는 원리는?\")\n",
    "print(content)\n",
    "print(docid)\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 메인 로직\n",
    "\n",
    "import json\n",
    "\n",
    "# 답변 데이터 생성\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    message = convertFormat_chain.invoke({\"message\": messages})\n",
    "    result = selectYn_chain.invoke({\"message\": message})\n",
    "\n",
    "    context = {\"message\":message}\n",
    "\n",
    "    if result == \"Y\":\n",
    "        context[\"reference\"], response[\"topk\"], response[\"references\"] = query_db(message)\n",
    "    else:\n",
    "        context[\"reference\"] = \"\"\n",
    "        response[\"topk\"] = []\n",
    "        response[\"references\"] = []\n",
    "    \n",
    "    response[\"answer\"] = answer_chain.invoke(context)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# 답변 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "\n",
    "            #if idx == 10: break\n",
    "            \n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 메인 \n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"./data/eval.jsonl\", \"./data/sample_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
